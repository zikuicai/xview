{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset\n",
    "\n",
    "\n",
    "GTCrossView images [here](https://www.mediafire.com/folder/f4gga3h86d659/GTCrossView)\n",
    "\n",
    "\n",
    "Refine_Net segmentation map [here](https://drive.google.com/file/d/1DXSSMOAUC5g8813k938v58oY-GvdSjJL/view)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will split the dayton images into train and test subsets according to the **AsBs** folder.\n",
    "\n",
    "pipeline:\n",
    "\n",
    "```\n",
    "read all images in train or test folder -> \n",
    "read the corresponding aerial and street images in the dayton folder -> \n",
    "concate them together -> \n",
    "put the resulting concat image into the new train or test folder\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/media/zikui/Data/xview_data'\n",
    "asbs_train_paths = os.path.join(data_root,'AsBs/train')\n",
    "asbs_test_paths = os.path.join(data_root,'AsBs/test')\n",
    "dayton_path = os.path.join(data_root,'dayton')\n",
    "\n",
    "asbs_train = glob.glob(os.path.join(asbs_train_paths, \"*.png\"))\n",
    "asbs_test = glob.glob(os.path.join(asbs_test_paths, \"*.png\"))\n",
    "dayton = glob.glob(os.path.join(dayton_path, \"*.jpg\"))\n",
    "\n",
    "asbs_train = sorted(asbs_train, key=lambda x: x.split('/')[-1])\n",
    "asbs_test = sorted(asbs_test, key=lambda x: x.split('/')[-1])\n",
    "dayton = sorted(dayton, key=lambda x: x.split('/')[-1])\n",
    "\n",
    "# sanity check \n",
    "print('dataset same size?',len(asbs_train) + len(asbs_test) == len(dayton)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train and test direcotries\n",
    "ABAsBs = os.path.join(data_root,'ABAsBs')\n",
    "ABAsBs_train = os.path.join(data_root,'ABAsBs/train')\n",
    "ABAsBs_test = os.path.join(data_root,'ABAsBs/test')\n",
    "os.makedirs(ABAsBs, exist_ok=False)\n",
    "os.makedirs(ABAsBs_train, exist_ok=False)\n",
    "os.makedirs(ABAsBs_test, exist_ok=False)\n",
    "train_index = 0\n",
    "test_index = 0\n",
    "# since the test dataset is too big, we only include 200 images from it.\n",
    "test_maxsize = 200\n",
    "\n",
    "# for i in range(100):\n",
    "for i in range(len(dayton)):\n",
    "    img_name = dayton[i].split('/')[-1]\n",
    "    if img_name.split('.')[-2] == 'overhead':\n",
    "        img_id = img_name[:-13]\n",
    "        print('img_id:',img_id)\n",
    "        img_A_path = dayton[i]\n",
    "        img_G_path = dayton[i+1]\n",
    "        img_A = cv.imread(img_A_path)\n",
    "        img_G = cv.imread(img_G_path)\n",
    "        size = (256,256)\n",
    "        img_A = cv.resize(img_A, size, interpolation = cv.INTER_AREA)\n",
    "        img_G = cv.resize(img_G, size, interpolation = cv.INTER_AREA)\n",
    "        img_GA = np.concatenate((img_G, img_A), axis=1)\n",
    "        \n",
    "        seg_train_name = asbs_train[0].split('/')[-1]\n",
    "        seg_train_id = seg_train_name[:-4]\n",
    "        seg_test_name = asbs_test[0].split('/')[-1]\n",
    "        seg_test_id = seg_test_name[:-4]\n",
    "        \n",
    "        if img_id == seg_train_id:\n",
    "            train_index += 1\n",
    "            seg_train = cv.imread(asbs_train[0])\n",
    "            img_GAs = np.concatenate((img_GA, seg_train), axis=1)\n",
    "            img_GAs_path = os.path.join(ABAsBs_train,str(train_index)+'.jpg')\n",
    "            cv.imwrite(img_GAs_path,img_GAs)\n",
    "            asbs_train.pop(0)\n",
    "            \n",
    "        elif test_index < test_maxsize and img_id == seg_test_id:\n",
    "            test_index += 1\n",
    "            seg_test = cv.imread(asbs_test[0])\n",
    "            img_GAs = np.concatenate((img_GA, seg_test), axis=1)\n",
    "            img_GAs_path = os.path.join(ABAsBs_test,str(test_index)+'.jpg')\n",
    "            cv.imwrite(img_GAs_path,img_GAs)\n",
    "            asbs_test.pop(0)\n",
    "            \n",
    "        else: raise NameError('img_id not train nor test!')\n",
    "#         overhead_name = dayton[i].split('/')[-1][:-13]\n",
    "#         print(overhead_name)\n",
    "#     elif img_name.split('.')[-2] == 'streetview':\n",
    "#         streetview_name = dayton[i].split('/')[-1][:-15]\n",
    "#         print(streetview_name)\n",
    "#     else: print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
